{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "mlops/model.py - MLflow pyfunc Model Wrapper.\n",
        "\n",
        "This module defines the `_PyfuncChatbot` class, a custom MLflow pyfunc model\n",
        "wrapper. This class enables the entire LLM service to be packaged, logged,\n",
        "and served using MLflow's standard model registry and serving capabilities.\n",
        "\"\"\"\n",
        "import json\n",
        "import asyncio\n",
        "import logging\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import mlflow.pyfunc\n",
        "\n",
        "# Set up logging for this module\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Import core components to be packaged with the model\n",
        "from core.service import LLMService\n",
        "from core.rag import RAGPipeline\n",
        "from core.router import HeuristicRouter\n",
        "from core.adapter import ControlHintAdapter\n",
        "from core.guardrail import Guardrail\n",
        "\n",
        "class _PyfuncChatbot(mlflow.pyfunc.PythonModel):\n",
        "    \"\"\"\n",
        "    A custom MLflow pyfunc model wrapper for the LLMService.\n",
        "\n",
        "    This class encapsulates the entire chatbot logic, including RAG, routing,\n",
        "    and guardrails, into a single, portable artifact that can be deployed\n",
        "    via MLflow's serving infrastructure.\n",
        "    \"\"\"\n",
        "    def load_context(self, context: mlflow.pyfunc.model.PythonModelContext):\n",
        "        \"\"\"\n",
        "        Loads the chatbot service and its components from the model artifacts.\n",
        "\n",
        "        This method is called once when the model is loaded for serving. It\n",
        "        initializes the RAG pipeline, adapter, router, and guardrail using\n",
        "        the artifacts stored during model registration.\n",
        "        \"\"\"\n",
        "        logger.info(\"Loading model context...\")\n",
        "\n",
        "        # Load artifacts: FAISS documents and the initial adapter hint\n",
        "        docs_path = context.artifacts[\"docs\"]\n",
        "        hint_path = context.artifacts[\"hint\"]\n",
        "        with open(docs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            docs = json.load(f)\n",
        "        with open(hint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            hint = f.read()\n",
        "\n",
        "        # Initialize core components\n",
        "        rag_pipeline = RAGPipeline(docs)\n",
        "        adapter = ControlHintAdapter(hint)\n",
        "        router = HeuristicRouter()\n",
        "        guardrail = Guardrail()\n",
        "\n",
        "        # Initialize the main LLMService\n",
        "        self.service = LLMService(\n",
        "            rag=rag_pipeline,\n",
        "            adapter=adapter,\n",
        "            router=router,\n",
        "            guardrail=guardrail\n",
        "        )\n",
        "        # Create a new event loop for async operations in the predict method\n",
        "        self.loop = asyncio.new_event_loop()\n",
        "\n",
        "        logger.info(\"Model context loaded successfully.\")\n",
        "\n",
        "    def predict(self, context: mlflow.pyfunc.model.PythonModelContext,\n",
        "                model_input: Union[pd.DataFrame, Dict[str, str]]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generates a chatbot response based on the input question.\n",
        "\n",
        "        This method is the main entry point for inference. It takes a DataFrame\n",
        "        or dictionary as input, extracts the questions, and uses the `LLMService`\n",
        "        to generate responses.\n",
        "\n",
        "        Args:\n",
        "            context: The MLflow context.\n",
        "            model_input: A pandas DataFrame or dictionary with a 'question' column.\n",
        "\n",
        "        Returns:\n",
        "            A list of answer strings corresponding to each question.\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting prediction...\")\n",
        "\n",
        "        if isinstance(model_input, dict):\n",
        "            # Allow for a single dictionary input\n",
        "            questions = [model_input.get(\"question\", \"\")]\n",
        "        elif isinstance(model_input, pd.DataFrame):\n",
        "            # Handle standard pandas DataFrame input\n",
        "            questions = model_input[\"question\"].tolist()\n",
        "        else:\n",
        "            raise TypeError(\"Model input must be a pandas DataFrame or a dictionary.\")\n",
        "\n",
        "        answers: List[str] = []\n",
        "        for q in questions:\n",
        "            # Run the async `answer` method from the LLMService\n",
        "            ans = self.loop.run_until_complete(self.service.answer(q))\n",
        "            answers.append(ans)\n",
        "\n",
        "        logger.info(\"Prediction complete.\")\n",
        "        return answers"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZzjFgy-gnqIe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}