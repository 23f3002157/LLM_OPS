{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "api/server.py - FastAPI Inference Server.\n",
        "\n",
        "This module provides the functionality to create and run a FastAPI server\n",
        "that serves the LLM chatbot model from the MLflow Model Registry. It defines\n",
        "the API endpoints and handles the loading of the model at startup.\n",
        "\"\"\"\n",
        "import os\n",
        "import logging\n",
        "from typing import Optional\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "from pydantic import BaseModel\n",
        "import mlflow.pyfunc\n",
        "\n",
        "# Set up logging for this module\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Pydantic models for API request and response bodies\n",
        "class ChatRequest(BaseModel):\n",
        "    \"\"\"Schema for the chatbot request body.\"\"\"\n",
        "    question: str\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    \"\"\"Schema for the chatbot response body.\"\"\"\n",
        "    answer: str\n",
        "    model: Optional[str] = None\n",
        "\n",
        "def create_app(model_name: str) -> FastAPI:\n",
        "    \"\"\"\n",
        "    Creates and configures the FastAPI application.\n",
        "\n",
        "    This function handles the application setup, including CORS middleware,\n",
        "    endpoint definitions, and most importantly, loading the production model\n",
        "    from MLflow at application startup.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model to serve from the MLflow registry.\n",
        "\n",
        "    Returns:\n",
        "        The configured FastAPI application instance.\n",
        "    \"\"\"\n",
        "    logger.info(\"Initializing FastAPI application...\")\n",
        "    app = FastAPI(title=\"LLMOps Chatbot API\", version=\"1.0.0\")\n",
        "\n",
        "    # Enable CORS for all origins to allow local development and broad access\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    # --- Application Startup Event ---\n",
        "    @app.on_event(\"startup\")\n",
        "    def load_model():\n",
        "        \"\"\"\n",
        "        Loads the production model from the MLflow Registry at startup.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading production model from MLflow: '{model_name}'...\")\n",
        "        try:\n",
        "            model_uri = f\"models:/{model_name}/Production\"\n",
        "            app.state.model = mlflow.pyfunc.load_model(model_uri)\n",
        "            logger.info(\"Model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load production model: {e}\", exc_info=True)\n",
        "            # Re-raise the exception to prevent the application from starting\n",
        "            raise RuntimeError(\"Application failed to start due to model loading error.\") from e\n",
        "\n",
        "    # --- API Endpoints ---\n",
        "    @app.get(\"/health\")\n",
        "    def health_check():\n",
        "        \"\"\"A simple health check endpoint.\"\"\"\n",
        "        return {\"status\": \"ok\"}\n",
        "\n",
        "    @app.post(\"/chat\", response_model=ChatResponse)\n",
        "    def chat_endpoint(req: ChatRequest):\n",
        "        \"\"\"\n",
        "        Main chat endpoint to receive a question and return a response.\n",
        "        \"\"\"\n",
        "        # The MLflow pyfunc model expects a pandas DataFrame or dict\n",
        "        result = app.state.model.predict({\"question\": req.question})\n",
        "\n",
        "        # The result is a list of strings, as defined by our pyfunc wrapper\n",
        "        answer = result[0] if isinstance(result, list) else str(result)\n",
        "\n",
        "        # Return the response, including the model name if available\n",
        "        return ChatResponse(\n",
        "            answer=answer,\n",
        "            model=os.getenv(\"MLFLOW_DEPLOYED_MODEL\", model_name)\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "def run_server(model_name: str, host: str, port: int):\n",
        "    \"\"\"\n",
        "    Runs the FastAPI server using Uvicorn.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model to serve.\n",
        "        host: The host address to bind the server to.\n",
        "        port: The port to listen on.\n",
        "    \"\"\"\n",
        "    app = create_app(model_name)\n",
        "    uvicorn.run(app, host=host, port=port)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_pnGO6zvn1ae"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}