{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "core/guardrail.py - The Guardrail Component.\n",
        "\n",
        "This module implements the `Guardrail` class, which is responsible for moderating\n",
        "user input and LLM responses to ensure they are safe and appropriate. It uses\n",
        "a pre-trained text classification model for this purpose.\n",
        "\"\"\"\n",
        "from transformers import pipeline\n",
        "\n",
        "class Guardrail:\n",
        "    \"\"\"\n",
        "    Moderates text content for toxicity and other inappropriate language.\n",
        "\n",
        "    The class uses a pre-trained `text-classification` model from the\n",
        "    Hugging Face Transformers library to score text. It's configured\n",
        "    to flag content as \"toxic\" if the confidence score exceeds a\n",
        "    predefined threshold.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the text classification pipeline.\"\"\"\n",
        "        self.moderator = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
        "\n",
        "    def check(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the provided text is safe.\n",
        "\n",
        "        Args:\n",
        "            text: The string to be moderated.\n",
        "\n",
        "        Returns:\n",
        "            True if the text is considered safe, False otherwise.\n",
        "        \"\"\"\n",
        "        # The moderator returns a list of dictionaries, e.g., [{'label': 'toxic', 'score': 0.99}]\n",
        "        result = self.moderator(text)[0]\n",
        "        # The threshold is set to 0.7 to avoid false positives\n",
        "        is_toxic = (result[\"label\"] == \"toxic\" and result[\"score\"] > 0.7)\n",
        "        return not is_toxic\n",
        "\n",
        "    def safe_response(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns a predefined safe response for when a query is flagged.\n",
        "        \"\"\"\n",
        "        return \"⚠️ Sorry, I cannot provide a response to that request.\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FMN6fPEHnOe7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}