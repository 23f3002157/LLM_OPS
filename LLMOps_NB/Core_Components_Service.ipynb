{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "core/service.py - The Core LLM Service Orchestrator.\n",
        "\n",
        "This module defines the `LLMService` class, which is the central hub for\n",
        "the chatbot's logic. It orchestrates all the core components to process a user\n",
        "query, generate a response, and log the entire process to MLflow.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import logging\n",
        "import mlflow\n",
        "from typing import List, Optional\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "from core.rag import RAGPipeline\n",
        "from core.router import HeuristicRouter\n",
        "from core.adapter import ControlHintAdapter\n",
        "from core.guardrail import Guardrail\n",
        "from config import CONFIG\n",
        "\n",
        "# Set up logging for this module\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LLMService:\n",
        "    \"\"\"\n",
        "    Orchestrates the LLM chatbot's core functionality.\n",
        "\n",
        "    This service is responsible for integrating the Guardrail, RAG, Adapter, and Router\n",
        "    components. It handles the end-to-end process of answering a user's query and\n",
        "    logging the results to MLflow for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self, rag: RAGPipeline, adapter: ControlHintAdapter,\n",
        "                 router: HeuristicRouter, guardrail: Guardrail):\n",
        "        \"\"\"\n",
        "        Initializes the LLMService with all necessary components.\n",
        "        \"\"\"\n",
        "        self.rag = rag\n",
        "        self.adapter = adapter\n",
        "        self.router = router\n",
        "        self.guardrail = guardrail\n",
        "\n",
        "    async def answer(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Processes a user query and returns a response from the LLM.\n",
        "\n",
        "        This method performs the following steps:\n",
        "        1. Checks the query with the guardrail.\n",
        "        2. Selects an LLM model using the router.\n",
        "        3. Retrieves relevant context from the RAG pipeline.\n",
        "        4. Adapts the prompt with the control hint.\n",
        "        5. Invokes the selected LLM and logs the result to MLflow.\n",
        "        6. Checks the LLM's response with the guardrail.\n",
        "        7. Returns the final response.\n",
        "\n",
        "        Args:\n",
        "            query: The user's question.\n",
        "\n",
        "        Returns:\n",
        "            The generated response string.\n",
        "        \"\"\"\n",
        "        # 1. Guardrail Check (on user query)\n",
        "        if not self.guardrail.check(query):\n",
        "            logger.warning(f\"Query flagged by guardrail: '{query}'\")\n",
        "            return self.guardrail.safe_response()\n",
        "\n",
        "        # 2. Select model\n",
        "        model = self.router.select_model(query)\n",
        "        client = ChatAnthropic(model=model, api_key=CONFIG.anthropic_api_key)\n",
        "\n",
        "        # 3. Retrieve context\n",
        "        context = self.rag.build_context(query)\n",
        "\n",
        "        # 4. Adapt prompt\n",
        "        prompt = self.adapter.adapt(f\"Context:\\n{context}\\n\\nQuestion: {query}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        with mlflow.start_run(nested=True, run_name=\"llm_inference\"):\n",
        "            try:\n",
        "                # 5. Invoke LLM and log\n",
        "                response = await client.ainvoke(prompt)\n",
        "                latency = time.time() - start_time\n",
        "                response_content = response.content\n",
        "\n",
        "                # 6. Guardrail Check (on LLM response)\n",
        "                if not self.guardrail.check(response_content):\n",
        "                    logger.warning(\"LLM response flagged by guardrail.\")\n",
        "                    mlflow.log_param(\"moderation_status\", \"flagged\")\n",
        "                    return self.guardrail.safe_response()\n",
        "\n",
        "                # Log all relevant information to MLflow\n",
        "                mlflow.log_param(\"model\", model)\n",
        "                mlflow.log_param(\"query\", query)\n",
        "                mlflow.log_metric(\"latency_ms\", latency * 1000)\n",
        "                mlflow.log_text(response_content, \"response.txt\")\n",
        "                mlflow.log_param(\"moderation_status\", \"passed\")\n",
        "\n",
        "                logger.info(f\"Response generated in {latency:.2f}s using {model}.\")\n",
        "                return response_content\n",
        "            except Exception as e:\n",
        "                mlflow.log_param(\"error\", str(e))\n",
        "                logger.error(f\"Failed to get response from LLM: {e}\", exc_info=True)\n",
        "                return \"[Error: An internal service error occurred.]\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "H7vTTZEEnk9N"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}