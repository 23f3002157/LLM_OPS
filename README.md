# ğŸ§  LLMOps Chatbot

This repository implements a **full-lifecycle LLMOps pipeline** for a production-grade chatbot.  
It covers **development, experimentation, human-in-the-loop feedback, model registration, and deployment** using modern MLOps best practices.

---

## ğŸš€ Features

- âœ… **Guardrails for Safety** â€“ Toxicity filtering with a Hugging Face **toxic-bert** model.  
- ğŸ” **Retrieval-Augmented Generation (RAG)** â€“ Document retrieval using **FAISS** for contextual responses.  
- ğŸ¤– **Dynamic LLM Routing** â€“ Intelligent **HeuristicRouter** selects the best model (e.g., GPT vs Claude 3.5) based on query context and feedback.  
- ğŸ“ **Prompt Adaptation** â€“ ControlHintAdapter dynamically adjusts system prompts based on feedback.  
- ğŸ“Š **Experimentation & Feedback** â€“ Human-in-the-Loop (HITL) feedback loop with adaptive response tuning.  
- ğŸ“¦ **MLflow Integration** â€“ End-to-end model packaging, versioning, and registry management.  
- ğŸŒ **FastAPI Inference Server** â€“ REST API with health and chat endpoints.  
- ğŸ³ **Docker + Kubernetes Ready** â€“ Containerized and scalable deployment workflow.  

---

## ğŸ§© Core Components

### 1. Guardrail  
Ensures safe operation by filtering both user queries and LLM responses for toxicity.  
- Uses **toxic-bert** from Hugging Face.  
- Returns a predefined safe response if content is unsafe.  

### 2. RAGPipeline  
- Retrieves relevant documents using **FAISS** similarity search.  
- Supplies retrieved context to the LLM for grounded answers.  

### 3. HeuristicRouter  
- Chooses the appropriate LLM dynamically.  
- Routing logic considers:
  - Query length  
  - `bias_towards_35` metric (adjusted based on user feedback)  

### 4. ControlHintAdapter  
- Dynamically modifies the LLM prompt.  
- Default hint: *"Answer in a helpful, concise, and knowledge-sharing way"*  
- Updated with stronger factual grounding when user gives negative feedback.  

### 5. LLMService  
The **orchestrator** that ties everything together:  
- Applies Guardrail checks  
- Runs RAG retrieval  
- Routes to an LLM  
- Applies prompt adaptation  
- Logs metrics (latency, model selection) via **MLflow**  

---

## ğŸ§ª Experimentation & HITL

The `chatbot_loop` function provides a CLI for interactive experimentation.  

- After each response, the user provides feedback:  
  - ğŸ‘ `y` â†’ Positive feedback  
  - ğŸ‘ `n` â†’ Negative feedback  
- Negative feedback triggers:  
  - **ControlHintAdapter update** â†’ "Be more factual, detailed, and knowledge-grounded"  
  - **Router bias adjustment** â†’ Favor **Claude-3.5-Sonnet**  

---

## ğŸ“¦ MLflow Model Packaging & Registry

- **_PyfuncChatbot**: Wraps `LLMService` into an MLflow `pyfunc` model.  
- Input: Pandas DataFrame / Dict with `"question"` column.  
- **Artifacts logged** with the model:
  - `docs.json` â†’ FAISS seed documents  
  - `adapter_hint.txt` â†’ Initial adapter hint  

### CLI Commands
- `register_model` â†’ Logs `_PyfuncChatbot` to MLflow Model Registry.  
- `promote_model` â†’ Promotes the latest version to **Staging** / **Production**.  

---

## ğŸŒ Production Deployment

A **FastAPI** app provides the production-ready inference layer.

### Endpoints
- `GET /health` â†’ Health check endpoint.  
- `POST /chat` â†’ Accepts `ChatRequest` (user question) â†’ Returns `ChatResponse` (answer).  

### Deployment
- **Dockerfile** â†’ Containerize the chatbot.  
- **Kubernetes manifest (K8S_DEPLOYMENT)** â†’ Scale & deploy on Kubernetes.  

---

## âš™ï¸ Tech Stack

- **LLMs**: OpenAI GPT, Anthropic Claude 3.5  
- **Safety**: Hugging Face `toxic-bert`  
- **Retrieval**: FAISS  
- **Experimentation**: MLflow (logging, registry, metrics)  
- **Serving**: FastAPI + Uvicorn  
- **Deployment**: Docker, Kubernetes  

---

## ğŸ Quickstart

```bash
# 1. Run chatbot loop for local testing
python chatbot.py

# 2. Register model in MLflow
python cli.py register_model --name llmops-chatbot

# 3. Promote to Production
python cli.py promote_model --name llmops-chatbot --stage Production

# 4. Launch FastAPI server
uvicorn server:app --host 0.0.0.0 --port 8000
